{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1 style=\"text-align: center;\">[Your Project Title]</h1>**\n",
    "\n",
    "**<h3 style=\"text-align: center;\">[Your Name]</h3>**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1. Business Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 Context**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The dataset contains customer information from a telecom company including demographics, services subscribed, account information, and churn status. The business context is to predict customer churn to enable targeted retention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Problem Statements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Which customers are likely to churn?\n",
    "- What factors influence customer churn the most?\n",
    "- How can the company reduce churn and improve customer retention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 Goals**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Develop a predictive model to classify customers as churn or no churn.\n",
    "- Identify key features affecting churn.\n",
    "- Provide actionable insights for business to reduce churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4 Analytical Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Use supervised machine learning classification techniques.\n",
    "- Data preprocessing including cleaning, feature engineering.\n",
    "- Model development with benchmarking, tuning, and evaluation.\n",
    "- Model explanation using feature importance and SHAP values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.5 Metric Evaluation (Business Metric, Machine Learning Evaluation Metric)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Business Metric: Reduction in churn rate.\n",
    "- Machine Learning Evaluation Metric: Accuracy, Precision, Recall, F1-score, ROC-AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.6 Success Criteria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Achieve high classification performance (e.g., ROC-AUC > 0.80).\n",
    "- Provide interpretable insights for business decisions.\n",
    "- Demonstrate model effectiveness on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 2. Data Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 General Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Dataset contains customer records with attributes related to demographics, services, contracts, billing, and churn.\n",
    "- Target variable: Churn (Yes/No)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings globally\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# Missing data visualization\n",
    "import missingno as msno\n",
    "\n",
    "# Statistical modeling and diagnostics\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning as SmConvergenceWarning\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Machine learning and preprocessing libraries\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, FunctionTransformer, StandardScaler\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Imbalanced data handling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "# Model selection and evaluation tools\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV, learning_curve\n",
    "from sklearn.metrics import (make_scorer, fbeta_score, classification_report, roc_auc_score, brier_score_loss,\n",
    "                             precision_recall_curve, RocCurveDisplay, roc_curve, f1_score)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "# Other utilities\n",
    "from scipy.stats import uniform\n",
    "import shap\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.read_csv(r'C:\\Users\\User\\Documents\\Purwadhika\\JCDS 2602\\Capstone Project\\telco-churn\\data\\raw\\data.csv')\n",
    "df = real_df.copy()\n",
    "\n",
    "print(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Feature Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature            | Description                                      | Impact to Business                                         |\n",
    "|--------------------|------------------------------------------------|------------------------------------------------------------|\n",
    "| customerID         | Unique customer identifier                       | Identification, no direct impact                            |\n",
    "| gender             | Customer gender (Male, Female)                   | Possible demographic influence                              |\n",
    "| SeniorCitizen      | Whether the customer is a senior citizen (0,1)  | May affect churn due to different needs                    |\n",
    "| Partner            | Whether customer has a partner (Yes, No)        | Social factors influencing churn                            |\n",
    "| Dependents         | Whether customer has dependents (Yes, No)       | Social factors influencing churn                            |\n",
    "| tenure             | Number of months customer has stayed             | Longer tenure usually means lower churn                     |\n",
    "| PhoneService       | Whether customer has phone service (Yes, No)    | Service usage influence                                     |\n",
    "| MultipleLines      | Whether customer has multiple lines (Yes, No, No phone service) | Service usage influence                                     |\n",
    "| InternetService    | Type of internet service (DSL, Fiber optic, No) | Service type impact on churn                                |\n",
    "| OnlineSecurity     | Whether customer has online security (Yes, No, No internet service) | Service feature impact                                      |\n",
    "| OnlineBackup       | Whether customer has online backup (Yes, No, No internet service) | Service feature impact                                      |\n",
    "| DeviceProtection   | Whether customer has device protection (Yes, No, No internet service) | Service feature impact                                      |\n",
    "| TechSupport        | Whether customer has tech support (Yes, No, No internet service) | Service feature impact                                      |\n",
    "| StreamingTV        | Whether customer streams TV (Yes, No, No internet service) | Service feature impact                                      |\n",
    "| StreamingMovies    | Whether customer streams movies (Yes, No, No internet service) | Service feature impact                                      |\n",
    "| Contract           | Contract type (Month-to-month, One year, Two year) | Contract length impact on churn                             |\n",
    "| PaperlessBilling   | Whether customer uses paperless billing (Yes, No) | Billing preference impact                                   |\n",
    "| PaymentMethod      | Payment method used                               | Billing method impact                                       |\n",
    "| MonthlyCharges     | Monthly charges                                  | Financial factor influencing churn                          |\n",
    "| TotalCharges       | Total charges to date                            | Financial factor influencing churn                          |\n",
    "| Churn              | Whether customer churned (Yes, No)              | Target variable                                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Missing Values Checking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Duplicated Values Checking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for duplicated customerID or rows; no explicit duplicates noted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 Dataset Restructuring for Better EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.select_dtypes(include=['object']).columns.tolist()\n",
    "df[categories] = df[categories].astype('category')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.6 Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.6.1 Outlier Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = df.select_dtypes(include=['float64','int64']).columns.tolist()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for i, col in enumerate(numerics):\n",
    "    sns.boxplot(x=df[col], orient='h', ax=axes[i])\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.6.2 Unique Values Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.select_dtypes(include=['category']).columns\n",
    "unique = dict()\n",
    "nunique = dict()\n",
    "for col in columns:\n",
    "    unique[col] = df[col].nunique()\n",
    "\n",
    "for col in columns: \n",
    "    nunique[col] = df[col].unique().tolist()\n",
    "\n",
    "unique_df = pd.DataFrame({'unique':unique, 'nunique':nunique})\n",
    "unique_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.6.3 Proportion Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_propo = df['Churn'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.pie(churn_propo, labels=churn_propo.index, autopct='%1.1f%%');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.6.4 Feature Distribution Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include='category').columns.drop('Churn')\n",
    "\n",
    "num_plots = len(categorical_cols)\n",
    "cols = 2\n",
    "rows = 8\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 10, rows * 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Compute counts per category and churn status with observed=True\n",
    "    counts = df.groupby([col, 'Churn'], observed=True).size().unstack(fill_value=0)\n",
    "\n",
    "    # Convert counts to percentages per category level (row-wise)\n",
    "    percentages = counts.div(counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Plot horizontal stacked bar chart on the subplot axis\n",
    "    percentages.plot(kind='barh', stacked=True, ax=ax, legend=False)\n",
    "\n",
    "    ax.set_title(f'{col}')\n",
    "    ax.set_xlabel('Percentage')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    # Annotate each segment with percentage and count\n",
    "    for j, (index, row) in enumerate(percentages.iterrows()):\n",
    "        cum_width = 0\n",
    "        for churn_status in percentages.columns:\n",
    "            pct = row[churn_status]\n",
    "            cnt = counts.loc[index, churn_status]\n",
    "            if pct > 0:\n",
    "                label = f'{pct:.1f}%\\n({cnt})'\n",
    "                ax.text(cum_width + pct / 2, j, label, ha='center', va='center', fontsize=8,\n",
    "                        color='white' if pct > 15 else 'black')\n",
    "                cum_width += pct\n",
    "\n",
    "# Remove any unused subplots\n",
    "for k in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[k])\n",
    "\n",
    "# Create one legend for all plots\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, title='Churn', loc='upper right')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])  # Leave space on right for legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.6.5 Correlation Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "# Compute Spearman correlation matrix for numeric columns\n",
    "corr = df.corr(numeric_only=True, method='spearman')\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Plot the heatmap with the mask applied to show only the lower triangle\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, mask=mask, annot=True, cmap='YlGnBu', vmin=-1, vmax=1, square=True, linewidths=0.5)\n",
    "plt.title('Spearman Correlation Heatmap (Lower Triangle)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.6.6 Correlation Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='Churn', corner=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.6.7 Statistics Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['object','category']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tenure ranges from 0 to 72 months.\n",
    "- MonthlyCharges vary widely, indicating different service usage.\n",
    "- Churn rate can be calculated from the 'Churn' column (proportion of 'Yes')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 3. Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalCharges'] = df['tenure'] * df['MonthlyCharges']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Target Labelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn target labeling\n",
    "df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Define X and y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable.\n",
    "X = df.drop(columns=['Churn']) \n",
    "y = df['Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split (stratify = y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5 Data Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns (excluding target 'Churn')\n",
    "categorical_cols = [col for col in df.select_dtypes(include=['category']).columns if col != 'Churn']\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Identify binary columns to map 'No' -> 0 and 'Yes' -> 1\n",
    "binary_cols = unique_df[unique_df['unique'] == 2].index.tolist()\n",
    "binary_cols = [col for col in binary_cols if col not in ['Churn']]\n",
    "\n",
    "# Separate categorical columns into binary and non-binary\n",
    "categorical_cols_no_binary = [col for col in categorical_cols if col not in binary_cols]\n",
    "\n",
    "# Function to map 'No' to 0 and 'Yes' to 1 in binary columns\n",
    "def map_binary_yes_no(X):\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X, columns=binary_cols)\n",
    "    for col in X.columns:\n",
    "        X[col] = X[col].map({'No': 0, 'Yes': 1}).astype(np.int64)\n",
    "    return X.values\n",
    "\n",
    "# Pipeline for numeric columns: imputation + scaling\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for binary columns: map Yes/No to 0/1\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    ('map_yes_no', FunctionTransformer(map_binary_yes_no))\n",
    "])\n",
    "\n",
    "# Pipeline for other categorical columns: one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "])\n",
    "\n",
    "# Combine all transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('binary', binary_transformer, binary_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols_no_binary)\n",
    "    ],\n",
    "    remainder='passthrough'  # pass through other columns and target\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 4. Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Initialize model pipeline and evaluation metrics.\n",
    "- Define custom metrics if needed.\n",
    "- Create a workflow of the experiment ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **4.2 Developing the Model Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Build machine learning pipeline including preprocessing and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized models dictionary with n_jobs where supported\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, n_jobs=-1),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'Bagging': BaggingClassifier(random_state=42, n_jobs=-1),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'k-NN': KNeighborsClassifier(n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(random_state=42, n_jobs=-1),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, n_jobs=-1),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=0, thread_count=-1)\n",
    "}\n",
    "\n",
    "# Use only recall scoring\n",
    "scoring = ['recall']\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "list_scores = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    for score in scoring:\n",
    "        scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring=score, n_jobs=-1)\n",
    "        list_scores.append({\n",
    "            'Model': model_name,\n",
    "            'Metric': score,\n",
    "            'Mean Score': np.mean(scores),\n",
    "            'Std Dev': np.std(scores)\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3 Model Benchmarking (Comparing model base performance)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Evaluate multiple algorithms (e.g., Logistic Regression, Random Forest, XGBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(list_scores)\n",
    "results_df = results_df.sort_values(by=['Mean Score', 'Std Dev'], ascending=False)\n",
    "results_df[results_df['Metric'] == 'recall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4 VIF Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After preprocessing, create a DataFrame of numeric features for VIF calculation\n",
    "# Apply the preprocessing pipeline to the training data (without the classifier)\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "# Get feature names after preprocessing\n",
    "def get_feature_names(preprocessor):\n",
    "    feature_names = []\n",
    "\n",
    "    # Numeric columns\n",
    "    feature_names.extend(numeric_cols)\n",
    "\n",
    "    # Binary columns (mapped to 0/1)\n",
    "    feature_names.extend(binary_cols)\n",
    "\n",
    "    # One-hot encoded categorical columns\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_features = ohe.get_feature_names_out(categorical_cols_no_binary)\n",
    "    feature_names.extend(cat_features)\n",
    "\n",
    "    return feature_names\n",
    "\n",
    "feature_names = get_feature_names(preprocessor)\n",
    "\n",
    "# Create DataFrame\n",
    "X_vif = pd.DataFrame(X_train_preprocessed, columns=feature_names)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X_vif.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "vif_data.sort_values(by='VIF', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.5 Tune Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Remove VIF Variables\n",
    "- Resampling using SMOTE/ADASYN/etc. (Using ImbLearn)\n",
    "- Hyperparameter tuning using grid search or randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to drop features with high VIF\n",
    "class VIFSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=10):\n",
    "        self.threshold = threshold\n",
    "        self.features_to_drop_ = []\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            X = X.toarray()\n",
    "        X_df = pd.DataFrame(X, columns=self.feature_names_)\n",
    "\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data['feature'] = X_df.columns\n",
    "        vif_vals = []\n",
    "        for i in range(X_df.shape[1]):\n",
    "            try:\n",
    "                vif = variance_inflation_factor(X_df.values, i)\n",
    "            except Exception:\n",
    "                vif = np.inf\n",
    "            vif_vals.append(vif)\n",
    "        vif_data['VIF'] = vif_vals\n",
    "\n",
    "        self.features_to_drop_ = vif_data[(vif_data['VIF'] == np.inf) | (vif_data['VIF'] > self.threshold)]['feature'].tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            X = X.toarray()\n",
    "        X_df = pd.DataFrame(X, columns=self.feature_names_)\n",
    "        X_reduced = X_df.drop(columns=self.features_to_drop_, errors='ignore')\n",
    "        return X_reduced.values\n",
    "\n",
    "    def set_feature_names(self, feature_names):\n",
    "        self.feature_names_ = feature_names\n",
    "        return self\n",
    "\n",
    "# Fungsi untuk mendapatkan feature names dari preprocessor secara dinamis\n",
    "def get_feature_names(preprocessor):\n",
    "    feature_names = []\n",
    "    feature_names.extend(numeric_cols)\n",
    "    feature_names.extend(binary_cols)\n",
    "    # Jika gender ada di pipeline dan diproses terpisah, tambahkan di sini jika perlu\n",
    "    if 'gender' in X_train.columns:\n",
    "        feature_names.append('gender')\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_features = ohe.get_feature_names_out(categorical_cols_no_binary)\n",
    "    feature_names.extend(cat_features)\n",
    "    return feature_names\n",
    "\n",
    "feature_names = get_feature_names(preprocessor)\n",
    "\n",
    "# Inisialisasi VIFSelector dengan feature names\n",
    "vif_selector = VIFSelector(threshold=50).set_feature_names(feature_names)\n",
    "\n",
    "# Logistic Regression model\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1)\n",
    "\n",
    "# Resamplers\n",
    "resamplers = {\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'ADASYN': ADASYN(random_state=42)\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Definisikan distribusi parameter dengan kondisi agar l1_ratio hanya untuk elasticnet\n",
    "param_dist = [\n",
    "    {\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['saga'],\n",
    "        # tidak ada l1_ratio di sini\n",
    "    },\n",
    "    {\n",
    "        'classifier__penalty': ['elasticnet'],\n",
    "        'classifier__solver': ['saga'],\n",
    "        'classifier__l1_ratio': uniform(0, 1)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Loop untuk tiap resampler\n",
    "for name, sampler in resamplers.items():\n",
    "    pipeline = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('vif_selector', vif_selector),\n",
    "        ('resampler', sampler),\n",
    "        ('classifier', logreg)\n",
    "    ])\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,\n",
    "        scoring='recall',\n",
    "        cv=cv,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        refit=True\n",
    "    )\n",
    "\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Cetak hasil terbaik\n",
    "    print(f\"Best recall score with {name}: {random_search.best_score_:.4f}\")\n",
    "    print(f\"Best parameters with {name}: {random_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.6 Analyze Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Evaluate model on test data.\n",
    "- Residual analysis and learning curves.\n",
    "- Learning Curve Inspection ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = random_search.best_estimator_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "y_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report on Test Data:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Residual analysis: plot histogram of residuals (y_test - predicted probabilities)\n",
    "residuals = y_test.astype(float) - y_proba\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(residuals, bins=30, edgecolor='k')\n",
    "plt.title(\"Residuals Histogram (y_true - predicted probabilities)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_pipeline, X_train, y_train,\n",
    "    cv=5, scoring='recall', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), random_state=42\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training recall')\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color='green', label='Testing recall')\n",
    "plt.title('Learning Curve (Recall)')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.7 Model Calibration (Classification Only)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Calibrate probabilities for better decision thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, roc_auc_score, roc_curve, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Probabilitas positif prediksi dari model terkalibrasi atau model terbaik\n",
    "y_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Prediksi kelas default dengan threshold 0.5\n",
    "y_pred_default = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "# Hitung recall score sebelum threshold tuning (default threshold 0.5)\n",
    "recall_before = recall_score(y_test, y_pred_default)\n",
    "print(f\"Recall Score before threshold tuning (threshold=0.5): {recall_before:.4f}\")\n",
    "\n",
    "# Hitung FPR, TPR, dan thresholds dari ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Hitung Youden’s J statistic (TPR - FPR)\n",
    "youden_j = tpr - fpr\n",
    "\n",
    "# Cari threshold dengan Youden’s J tertinggi\n",
    "best_idx = youden_j.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "# Hitung ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"Best threshold by Youden’s J statistic: {best_threshold:.4f}\")\n",
    "print(f\"TPR: {tpr[best_idx]:.4f}, FPR: {fpr[best_idx]:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Gunakan threshold ini untuk prediksi kelas baru\n",
    "y_pred_new = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Hitung recall score dengan threshold baru\n",
    "recall_after = recall_score(y_test, y_pred_new)\n",
    "print(f\"Recall Score after threshold tuning (threshold={best_threshold:.4f}): {recall_after:.4f}\")\n",
    "\n",
    "# Evaluasi ulang dengan threshold baru\n",
    "# print(\"Classification Report with adjusted threshold:\")\n",
    "# print(classification_report(y_test, y_pred_new))\n",
    "\n",
    "# Visualisasi ROC curve dengan threshold terpilih\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.scatter(fpr[best_idx], tpr[best_idx], color='red', label=f'Best threshold={best_threshold:.4f}')\n",
    "plt.plot([0,1],[0,1],'--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve with Selected Threshold')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, recall_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Probabilitas positif prediksi dari model terkalibrasi atau model terbaik\n",
    "y_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Prediksi kelas default dengan threshold 0.5\n",
    "y_pred_default = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "# Hitung recall score sebelum threshold tuning (default threshold 0.5)\n",
    "recall_before = recall_score(y_test, y_pred_default)\n",
    "print(f\"Recall Score before threshold tuning (threshold=0.5): {recall_before:.4f}\")\n",
    "\n",
    "# Hitung precision, recall, dan thresholds dari PR curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Hitung F1-score untuk setiap threshold\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)  # hindari pembagian 0\n",
    "\n",
    "# Cari threshold dengan F1-score tertinggi\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"Best threshold by F1-score on PR curve: {best_threshold:.4f}\")\n",
    "print(f\"Precision: {precision[best_idx]:.4f}, Recall: {recall[best_idx]:.4f}, F1-score: {f1_scores[best_idx]:.4f}\")\n",
    "\n",
    "# Gunakan threshold ini untuk prediksi kelas baru\n",
    "y_pred_new = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Hitung recall score dengan threshold baru\n",
    "recall_after = recall_score(y_test, y_pred_new)\n",
    "print(f\"Recall Score after threshold tuning (threshold={best_threshold:.4f}): {recall_after:.4f}\")\n",
    "\n",
    "# Evaluasi kembali menggunakan threshold baru\n",
    "# print(\"Classification Report with adjusted threshold:\")\n",
    "# print(classification_report(y_test, y_pred_new))\n",
    "\n",
    "# Visualisasi PR Curve dengan threshold terpilih\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(recall, precision, label='PR Curve')\n",
    "plt.scatter(recall[best_idx], precision[best_idx], color='red', label=f'Best threshold={best_threshold:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve with Selected Threshold')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 5. Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1 Initial vs Final Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, recall_score\n",
    "\n",
    "baseline_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit baseline pipeline\n",
    "baseline_pipe.fit(X_train, y_train)\n",
    "\n",
    "# 1. Prediksi dan evaluasi model baseline (asumsi baseline_pipe sudah fit)\n",
    "y_proba_baseline = baseline_pipe.predict_proba(X_test)[:, 1]\n",
    "roc_auc_baseline = roc_auc_score(y_test, y_proba_baseline)\n",
    "print(f\"Baseline Logistic Regression ROC AUC: {roc_auc_baseline:.4f}\")\n",
    "\n",
    "# 2. Prediksi dan evaluasi model hasil hyperparameter tuning (random_search sudah fit)\n",
    "best_pipe = random_search.best_estimator_\n",
    "y_proba_tuned = best_pipe.predict_proba(X_test)[:, 1]\n",
    "roc_auc_tuned = roc_auc_score(y_test, y_proba_tuned)\n",
    "print(f\"Tuned Logistic Regression ROC AUC: {roc_auc_tuned:.4f}\")\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "# 3. Threshold adjustment berdasarkan ROC curve (Youden’s J statistic)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_tuned)\n",
    "youden_j = tpr - fpr\n",
    "best_idx = youden_j.argmax()\n",
    "optimal_threshold = thresholds[best_idx]\n",
    "print(f\"Optimal threshold from ROC curve: {optimal_threshold:.4f}\")\n",
    "\n",
    "# Prediksi kelas dengan threshold optimal\n",
    "y_pred_adjusted = (y_proba_tuned >= optimal_threshold).astype(int)\n",
    "\n",
    "# ROC AUC tetap sama karena threshold tidak ubah probabilitas\n",
    "roc_auc_adjusted = roc_auc_score(y_test, y_proba_tuned)\n",
    "print(f\"ROC AUC after threshold adjustment (same as tuned): {roc_auc_adjusted:.4f}\")\n",
    "\n",
    "# Contoh hitung recall dengan threshold default dan threshold optimal\n",
    "y_pred_default = (y_proba_tuned >= 0.5).astype(int)\n",
    "recall_default = recall_score(y_test, y_pred_default)\n",
    "recall_adjusted = recall_score(y_test, y_pred_adjusted)\n",
    "\n",
    "print(f\"Recall with default threshold 0.5: {recall_default:.4f}\")\n",
    "print(f\"Recall with optimal threshold {optimal_threshold:.4f}: {recall_adjusted:.4f}\")\n",
    "\n",
    "# Ringkasan hasil\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Baseline ROC AUC: {roc_auc_baseline:.4f}\")\n",
    "print(f\"Tuned ROC AUC: {roc_auc_tuned:.4f}\")\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"Recall (default threshold): {recall_default:.4f}\")\n",
    "print(f\"Recall (optimal threshold): {recall_adjusted:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 Model Explanation and Interpretation Using Statsmodels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Transform the training data with the pipeline (preprocessor + VIF selector)\n",
    "# Fit preprocessor on training data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Function to get feature names after preprocessing\n",
    "def get_feature_names(preprocessor):\n",
    "    feature_names = []\n",
    "    feature_names.extend(numeric_cols)\n",
    "    feature_names.extend(binary_cols)\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_features = ohe.get_feature_names_out(categorical_cols_no_binary)\n",
    "    feature_names.extend(cat_features)\n",
    "    return feature_names\n",
    "\n",
    "feature_names = get_feature_names(preprocessor)\n",
    "\n",
    "# Convert preprocessed data to DataFrame with feature names\n",
    "X_train_df = pd.DataFrame(X_train_preprocessed, columns=feature_names)\n",
    "\n",
    "# Apply VIFSelector transform to remove multicollinear features\n",
    "vif_selector = VIFSelector(threshold=50).set_feature_names(feature_names)\n",
    "vif_selector.fit(X_train_df)\n",
    "X_train_reduced = vif_selector.transform(X_train_df)\n",
    "\n",
    "# Get final feature names after removing high VIF features\n",
    "features_after_vif = [f for f in feature_names if f not in vif_selector.features_to_drop_]\n",
    "\n",
    "# Create DataFrame for final features\n",
    "X_train_final = pd.DataFrame(X_train_reduced, columns=features_after_vif)\n",
    "\n",
    "# --- IMPORTANT: Align indices of features and target ---\n",
    "# Option 1: Reset indices of both\n",
    "X_train_final = X_train_final.reset_index(drop=True)\n",
    "y_train_reset = y_train.reset_index(drop=True)\n",
    "\n",
    "# 2. Add intercept column for statsmodels\n",
    "X_train_final = sm.add_constant(X_train_final)\n",
    "\n",
    "# 3. Fit logistic regression using statsmodels (unregularized for interpretation)\n",
    "model_sm = sm.Logit(y_train_reset, X_train_final)\n",
    "result = model_sm.fit(disp=False)\n",
    "\n",
    "# 4. Print summary\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.3 Feature Importances Using SHAP Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best pipeline\n",
    "best_pipeline = random_search.best_estimator_\n",
    "\n",
    "# Preprocessor + VIF selector pipeline\n",
    "preprocessor_vif = ImbPipeline([\n",
    "    ('preprocessor', best_pipeline.named_steps['preprocessor']),\n",
    "    ('vif_selector', best_pipeline.named_steps['vif_selector'])\n",
    "])\n",
    "\n",
    "# Transform test data\n",
    "X_test_preprocessed = preprocessor_vif.transform(X_test)\n",
    "\n",
    "# Get feature names after preprocessing and VIF filtering\n",
    "feature_names = get_feature_names(best_pipeline.named_steps['preprocessor'])\n",
    "features_after_vif = [f for f in feature_names if f not in best_pipeline.named_steps['vif_selector'].features_to_drop_]\n",
    "\n",
    "# Get classifier\n",
    "model = best_pipeline.named_steps['classifier']\n",
    "\n",
    "# Create SHAP explainer and values\n",
    "explainer = shap.LinearExplainer(model, X_test_preprocessed, feature_perturbation=\"interventional\")\n",
    "shap_values = explainer.shap_values(X_test_preprocessed)\n",
    "\n",
    "# SHAP summary plot\n",
    "shap.summary_plot(shap_values, features=X_test_preprocessed, feature_names=features_after_vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.3 Counterfactual analysis for churn prediction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Hitung confusion matrix dengan prediksi threshold optimal\n",
    "cm = confusion_matrix(y_test, y_pred_adjusted)\n",
    "\n",
    "# Label kelas\n",
    "class_labels = ['not churn', 'churn']\n",
    "\n",
    "# Buat figure\n",
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "# Plot heatmap tanpa anotasi angka dulu\n",
    "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=[f'Predicted {label}' for label in class_labels],\n",
    "            yticklabels=[f'Actual {label}' for label in class_labels])\n",
    "\n",
    "# Tambahkan anotasi angka dan label TP, TN, FP, FN\n",
    "thresh = cm.max() / 2.\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        # Tentukan label TP, TN, FP, FN berdasarkan posisi matriks\n",
    "        if i == 0 and j == 0:\n",
    "            label = f'TN\\n{cm[i, j]}'\n",
    "        elif i == 1 and j == 1:\n",
    "            label = f'TP\\n{cm[i, j]}'\n",
    "        elif i == 0 and j == 1:\n",
    "            label = f'FP\\n{cm[i, j]}'\n",
    "        else:  # i == 1 and j == 0\n",
    "            label = f'FN\\n{cm[i, j]}'\n",
    "        \n",
    "        plt.text(j + 0.5, i + 0.5, label,\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix - Final Model (Threshold Optimal)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 6. Deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.1 Model Deployment Using Joblib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2 How To Use Joblib Model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3 Model Limitations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Model depends on data quality and feature relevance.\n",
    "- Changes in customer behavior or services may reduce model accuracy over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 7. Conclusion and Recommendation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.1 Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Model: The developed model predicts churn with good accuracy and provides insights into key factors.\n",
    "- Business: Understanding churn drivers enables targeted retention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.2 Recommendation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Model: Regularly update and monitor model performance.\n",
    "- Business: Use model insights to improve customer service, contract offerings, and billing options to reduce churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
